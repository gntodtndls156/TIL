#개발일지 #AI

## <mark style="background: #ABF7F7A6;">Background</mark>

> [!info] 과제 내용
> **사회적인 문제를 해결하는데 도움이 될 수 있는 주제를 선정하여 인공지능 프로그램을 개발하고 보고서와 소스코드를 제출하세요.**

청각 장애가 있는 본인은 수업 내용을 완전히 이해하지 못 하여, 인공지능에 대한 이해도가 낮습니다. ChatGPT와 인터넷을 최대한 활용해 프로그램을 만들어 보고자 합니다.

또한 인공지능 프로그램을 만들 때 어떻게 개발했는지 기록할 것입니다.

## 과정

### 기본

간단한 선형 회귀 모델을 구현하여 데이터를 학습하고 예측하는 코드입니다.

```python title:sample
# 필요한 라이브러리 불러오기
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 예제 데이터 생성
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 데이터 시각화
plt.scatter(X, y)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Sample Data')
plt.show()

# 데이터 분할 (학습 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 선형 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)

# 테스트 데이터에 대한 예측
y_pred = model.predict(X_test)

# 모델 성능 평가
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# 결과 시각화
plt.scatter(X_test, y_test, label='Actual')
plt.scatter(X_test, y_pred, label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Results')
plt.legend()
plt.show()
```

### 주제 정하기

텍스트 데이터를 기반으로 문장의 감정을 분석하는 프로그램을 만들어 보기로 결정했습니다. 

요즘 사회는 SNS 또는 메시지를 통해 커뮤니티케이션이 이루어집니다. 
코로나 시절 비대면으로 대화하려니 상대방이 얼굴 표정을 어떻게 지으면서 문장을 작성하였는가?, 문장에 어떤 감정이 실렸는지, 장난으로 한 말인지, 사실대로 말한 것인지 알 수가 없었습니다.
그런 계기로 서로 오해하기가 쉽상이고 관계가 모래성 같이 무너지기도 합니다. 

- 기본적인 기능:
	1. 주어진 텍스트나 문장을 입력 받습니다.
	2. 인공지능 프로그램이 그 문장을 분석합니다.
	3. 이 문장에 부정적이거나 긍정적인 감정이 실었는지 판정하고 결과를 출력합니다.

### 라이브러리 선택

> 딥러닝 프레임워크

`TensorFlow` 또는 `PyTorch`가 있다고 합니다. 학교에서 `TenforFlow` 가르쳤기 때문에 이것을 활용하기로 결정했습니다.

또한, `Tensorflow`는 기본적으로 다양한 자연어 처리 작업에 사용할 수 있는 다양한 기능과 모듈을 제공하므로 자연어 처리 라이브러리를 따로 채택하지 않았습니다.

`Tensorflow`의 NLP를 `Sequential` 모델로 채택했습니다.
레이어를 선형으로 쌓은 구조를 갖추고, 각 레이어는 이전 레이어의 출력을 입력으로 받는 특징이 있습니다. 

> 설치

Anaconda 프로그램으로 설치했습니다. 따라서 `pip`명령어가 아닌 `conda`명령어를 활용했다는 점 주의하세요.

```shell title:LibarayInstall
[pip|conda] install tensorflow
```

### 자연어 처리할 재료

이 글이 부정적인지 긍정적인지 판단하는 재료가 필요합니다. 별표가 낮으면 부정적인 글일 것이고, 높으면 높을 수록 긍정적인 글일 것입니다. 그러므로, IMDB 리뷰와 같은 영화 또는 애플리케이션의 리뷰를 가져와 재료로 쓰이면 될 것입니다.

IMDB Dataset을 가져와야 하므로 tensorflow 라이브러리를 사용합니다. → [imdb_reviews  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/imdb_reviews?hl=ko)

Dataset은 대형 영화 리뷰 데이터 세트이며, 이진 감정 분류용 Dataset입니다. 교육용으로 25,000개의 극단적인 영화 리뷰 세트와 테스트용으로 25,000개의 세트를 제공하고, 레이블이 지정되지 않은 추가 데이터도 사용할 수 있습니다.

![[Pasted image 20231216200631.png]]

### 기초 코드

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb
```

```python title:Loding_data
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
```

`num_words=10000`은 빈도가 가장 높은 10,000개의 단어만을 사용하도록 데이터를 제한하여, 불필요한 정보를 제거하고 일반화 능력을 향상시킵니다. 

```python
train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=256)
test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=256)
```

`pad_sequences`함수를 사용하여 각 리뷰의 길이를 동일하게 만들어줍니다. 모든 리뷰를 `maxlen`을 통해 256으로 패딩하고 부족한 부분은 0으로 채웁니다.

```python
model = models.Sequential()
model.add(layers.Embedding(input_dim=10000, output_dim=16, input_length=256))
model.add(layers.GlobalAveragePooling1D())
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

딥러닝 모델을 정의하는 부분입니다. 

Embedding 레이어를 사용하여 단어를 밀집 벡터로 변환하고, `GlobalAveragePooling1D`를 통해 고정된 길이의 벡터로 변환합니다. 그 후에 Fully Connected 레이어를 추가하여 모델의 복잡성을 증가시키고, 최종 출력은 이진 분류를 위한 시그모이드 활성화 함수를 사용합니다.

```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

모델을 컴파일하는 부분입니다.

`adam` 옵티마이저를 사용하고, 손실 함수로는 이진 분류에 사용되는 ‘binary_crossentropy’를 사용합니다. 성능 측정을 위해 정확도 `accuracy`를 사용합니다.

```python
model.fit(train_data, train_labels, epochs=10, validation_split=0.2, batch_size=512)
```

모델을 훈련하는 부분입니다.

`fit` 메서드를 사용하여 훈련 데이터를 학습하고, 10 에포크 동안 훈련을 진행합니다. 훈련 데이터 중 20%를 검증 데이터로 사용하며, 배치 크기는 512로 설정됩니다.

```python
test_loss, test_acc = model.evaluate(test_data, test_labels)
print(f'Test Accuracy: {test_acc * 100:.2f}%')
```

훈련된 모델을 평가하는 부분입니다.

테스트 데이터를 사용하여 모델의 성능을 평가하고, 정확도를 출력합니다.

### 개선 코드

> 에포크 조정

`epochs = 10`로 조정한 결과는 `Test Accuracy: 86.06%` 입니다.
`epochs = 15`로 조정한 결과는 `Test Accuracy: 87.51%` 입니다.
`epochs = 16`로 조정한 결과는 `Test Accuracy: 87.74%` 입니다.
`epochs = 20`로 조정한 결과는 `Test Accuracy: 87.88%` 입니다.
`epochs = 21`로 조정한 결과는 `Test Accuracy: 87.91%` 입니다.
`epochs = 25`로 조정한 결과는 `Test Accuracy: 87.97%` 입니다.

에포크의 숫자가 높아질 수록 정확률이 유의미하게 높아지지 않게 됩니다. 
오히려 에포크의 숫자가 `30`인 경우 **정확률이 떨어지는 것을 확인**할 수 있었습니다. 
`Test Accuracy: 87.78%`

> 다양한 최적화 알고리즘 사용

**SGD**: `epochs = 10`의 경우 `Test Accuracy: 50.02%`이고, `epochs = 25`의 경우 `Test Accuracy: 49.99%`가 나왔습니다.
**SGD with momentum**: `epochs = 10`, `momentum=0.9`의 경우 `Test Accuracy: 53.87%`가 나왔습니다.

**Adagrad**: `epochs = 10`의 경우 `Test Accuracy: 51.34%`이고, `epochs = 25`의 경우 `Test Accuracy: 52.76%`입니다. 그렇게 유의미하게 상승하지 않았습니다.

**Adamax**: `epochs = 25`의 경우 `Test Accuracy: 86.15%`가 나왔습니다.

이로써 가장 높은 정확률이 나와준 알고리즘은 **Adam**인 것으로 확인했습니다.

> Dropout과 뉴런 레이어 추가

```python
model.add(layers.Dense(64, activation='relu')) 
model.add(layers.Dropout(0.5))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dropout(0.5))
```

결과: `Test Accuracy: 85.58%`

`epochs = 10` 의 경우 `Test Accuracy: 87.68%`
`epochs = 9`의 경우 `Test Accuracy: 87.81%`

> 학습률 조정

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
```

`optimizer` 변수를 추가했습니다.

`epochs = 15`으로 `val_loss` 모니터링 해 본 결과, `8`가 가장 적은 것으로 확인되어 조정 후 결과를 봅니다. `epochs = 8`의 경우 `Test Accuracy: 88.01%`가 나와주었습니다.

## 결과

```python title:최종
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb

# 데이터 로드
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

# 데이터 전처리: 시퀀스 패딩
train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=256)
test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=256)

# 모델 정의
model = models.Sequential()
model.add(layers.Embedding(input_dim=10000, output_dim=16, input_length=256))
model.add(layers.GlobalAveragePooling1D())

model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))

# 모델 컴파일
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) # adam

# 모델 훈련
model.fit(train_data, train_labels, epochs=8, validation_split=0.2, batch_size=512) # 25

# 모델 평가
test_loss, test_acc = model.evaluate(test_data, test_labels)
print(f'Test Accuracy: {test_acc * 100:.2f}%')
```

![[Pasted image 20231217130315.png]]